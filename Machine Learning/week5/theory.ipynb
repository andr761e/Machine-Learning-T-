{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: Break Points and Growth Functions \n",
    "\n",
    "-   Is there always a break point for a finite hypothesis set of $n$\n",
    "    hypotheses? If so, can you give a an upper bound? What is the growth\n",
    "    function?\n",
    "\n",
    "-   Does the set of all functions have a break point? What is its growth\n",
    "    function?\n",
    "\n",
    "-   What is the (smallest) break point for the hypothesis set consisting\n",
    "    of circles centered around $(0,0)$? For a given circle the\n",
    "    hypothesis returns $1$ for points inside the circle and $-1$ for\n",
    "    points outside. What is the growth function?\n",
    "\n",
    "-   What if we move to centered balls in the 3-dimensional space\n",
    "    ${{\\mathbb R}}^3$? Or in general $d$-dimensional space\n",
    "    ${{\\mathbb R}}^d$ (hyperspheres)?\n",
    "\n",
    "-  Show that the growth function for a singleton hypothesis class $H = \\{h\\}$ is 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 solution\n",
    "- Yes, for a finite hypothesis set of $n$ hypotheses, there is always a break point. The break point is the smallest number $k$ such that no set of $k$ points can be shattered by the hypothesis set. An upper bound on the break point is $k \\leq n+1$, where $n$ is the size of the hypothesis set. The growth function is the maximum number of dichotomies (distinct ways of labeling) that can be realized on any set of $m$ points. For a finite hypothesis set with $n$ hypotheses, the growth function is at most $n$, i.e., it is the number of hypotheses, meaning it cannot shatter more than $n$ points.\n",
    "\n",
    "- No, the set of all functions does not have a break point. This is because the set of all possible functions can shatter any number of points, meaning it can realize all possible dichotomies for any set of points. The growth function of the set of all functions is $2^m$, where $m$ is the number of points. This is the maximum possible number of dichotomies for any set of $m$ points.\n",
    "\n",
    "- For the hypothesis set of circles centered at (0,0) in the 2D plane, the smallest break point is 4. This is because you cannot shatter 4 points in the plane with a set of circles centered at the origin. While you can arrange 3 points in such a way that all dichotomies can be achieved (i.e., they can be shattered), this is not possible for 4 points.\n",
    "\n",
    "- In the case of centered balls (spheres) in $\\mathbb{R}$, the smallest break point would be 5. You can shatter 4 points with spheres, but you cannot shatter 5 points.\n",
    "In general, for balls (hyperspheres) in $d$-dimensional space $\\mathbb{R}^d$, the smallest break point is $d+2$. This is because you can shatter $d+1$ points but not $d+2$ points.\n",
    "The growth function in general $d$-dimensional space grows as $m^d$, meaning it is polynomial in $m$, the number of points, and the exponent of the polynomial is related to the dimensionality $d$.\n",
    "\n",
    "- For a singleton hypothesis class $H = \\{h\\}$, which contains only a single hypothesis, the growth function is trivially 1. This is because, for any number of points, there is only one hypothesis in the class, and thus only one possible labeling (dichotomy). Therefore, no matter how many points we have, the number of realizable dichotomies is always 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: VC Dimension \n",
    "\n",
    "-   Does VC Dimension depend on the learning algorithm or the actual\n",
    "    data set given?\n",
    "\n",
    "-   Does VC Dimension depend on the probability distribution generating\n",
    "    the data (not the labels)?\n",
    "\n",
    "-   If $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$ is\n",
    "    $VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)?$\n",
    "\n",
    "-   Can you give an upper bound on the VC dimension of a finite set of\n",
    "    $M$ hypotheses?\n",
    "\n",
    "-   What is the VC Dimension for the hypothesis set consisting of\n",
    "    circles centered around 0?\n",
    "\n",
    "-   What if we move to balls (3d)? or in general d dimensions\n",
    "    (hyperspheres)?\n",
    "\n",
    "-   What is the maximal VC dimension possible of the intersection of\n",
    "    hypothesis sets $\\mathcal{H}_1,\\dots,\\mathcal{H}_n$ with VC\n",
    "    dimension $v_1,\\dots,v_n$.\n",
    "\n",
    "-   As previous question, instead what is the minimal VC dimension of\n",
    "    the union of the hypothesis srets from the previous question\n",
    "\n",
    "-   Show that the VC dimension of the hypothesis set consisting of axis aligned rectangles in $\\mathbb{R}^2$ is 4,\n",
    "    i.e. find a point set of 4 points you can shatter and argue that any point set of size 5 can not.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 solution\n",
    "\n",
    "### 1. Does VC Dimension depend on the learning algorithm or the actual data set given?\n",
    "\n",
    "No, the **VC dimension** does **not depend** on the learning algorithm or the actual data set. It is a property of the hypothesis class itself and represents the capacity or expressive power of the hypothesis class, independent of how learning is conducted or the specific data being used.\n",
    "\n",
    "### 2. Does VC Dimension depend on the probability distribution generating the data (not the labels)?\n",
    "\n",
    "No, the **VC dimension** does not depend on the probability distribution generating the data. It only depends on the hypothesis class and how well it can shatter different point sets. It is a combinatorial property that does not change based on the underlying probability distribution of the data.\n",
    "\n",
    "### 3. If \\(\\mathcal{H}_1 \\subseteq \\mathcal{H}_2\\), is \\(VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)\\)?\n",
    "\n",
    "Yes, if \\(\\mathcal{H}_1 \\subseteq \\mathcal{H}_2\\), then \\(VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)\\). This is because \\(\\mathcal{H}_2\\) contains all the hypotheses in \\(\\mathcal{H}_1\\) and potentially more. Hence, the hypothesis class \\(\\mathcal{H}_2\\) can shatter at least as many points as \\(\\mathcal{H}_1\\), and possibly more.\n",
    "\n",
    "### 4. Can you give an upper bound on the VC dimension of a finite set of \\(M\\) hypotheses?\n",
    "\n",
    "Yes, for a finite set of \\(M\\) hypotheses, the VC dimension is at most \\(\\log_2 M\\). This is because, in the worst case, each hypothesis corresponds to a different label on a set of points, and the number of distinct labelings (dichotomies) is bounded by \\(M\\). Therefore, the VC dimension cannot exceed \\(\\log_2 M\\), which is the number of points that can be shattered by \\(M\\) distinct hypotheses.\n",
    "\n",
    "### 5. What is the VC Dimension for the hypothesis set consisting of circles centered around 0?\n",
    "\n",
    "The **VC dimension** for the hypothesis set consisting of circles centered around \\(0\\) in the 2D plane is **3**. This is because you can shatter 3 points with circles but cannot shatter 4 points. Hence, the VC dimension for this hypothesis set is 3.\n",
    "\n",
    "### 6. What if we move to balls (3D)? Or in general \\(d\\) dimensions (hyperspheres)?\n",
    "\n",
    "In 3D (balls), the **VC dimension** is 4. You can shatter 4 points using balls but cannot shatter 5 points.\n",
    "\n",
    "For general \\(d\\)-dimensional space (hyperspheres), the VC dimension is \\(d+1\\). This is because you can shatter \\(d+1\\) points using hyperspheres, but you cannot shatter \\(d+2\\) points.\n",
    "\n",
    "### 7. What is the maximal VC dimension possible of the intersection of hypothesis sets \\(\\mathcal{H}_1, \\dots, \\mathcal{H}_n\\) with VC dimension \\(v_1, \\dots, v_n\\)?\n",
    "\n",
    "The maximal VC dimension of the intersection of hypothesis sets \\(\\mathcal{H}_1, \\dots, \\mathcal{H}_n\\) is at most the **minimum VC dimension** among them. That is,\n",
    "\n",
    "\\[\n",
    "VC(\\mathcal{H}_1 \\cap \\dots \\cap \\mathcal{H}_n) \\leq \\min(v_1, \\dots, v_n)\n",
    "\\]\n",
    "\n",
    "This is because the intersection of the hypothesis sets can only shatter as many points as the least expressive hypothesis set in the intersection.\n",
    "\n",
    "### 8. As previous question, instead what is the minimal VC dimension of the union of the hypothesis sets from the previous question?\n",
    "\n",
    "The **VC dimension of the union** of the hypothesis sets can be upper-bounded by the sum of the VC dimensions of the individual sets:\n",
    "\n",
    "\\[\n",
    "VC(\\mathcal{H}_1 \\cup \\dots \\cup \\mathcal{H}_n) \\leq v_1 + \\dots + v_n\n",
    "\\]\n",
    "\n",
    "This is because the union of the hypothesis sets can potentially shatter more points than any individual set, but is still limited by the combined capacity of all the sets.\n",
    "\n",
    "### 9. Show that the VC dimension of the hypothesis set consisting of axis-aligned rectangles in \\(\\mathbb{R}^2\\) is 4, i.e., find a point set of 4 points you can shatter and argue that any point set of size 5 cannot.\n",
    "\n",
    "- **Shattering 4 points**: Consider 4 points arranged in a rectangular grid. You can place axis-aligned rectangles in such a way that for any labeling of the 4 points, there exists a rectangle that realizes that labeling. This shows that the hypothesis set can shatter 4 points.\n",
    "\n",
    "- **Failure to shatter 5 points**: When you add a fifth point, there will be some labelings of the 5 points that cannot be achieved using axis-aligned rectangles. This is because axis-aligned rectangles cannot accommodate every possible configuration of 5 points (e.g., isolating one point in the center with a label that is different from the surrounding points). Therefore, the VC dimension is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3:  Book Exercise\n",
    "### Exercise 1.11 in the [LFD] Book \n",
    "(Not problem, but exercise inside the text. page 25) <b>Hint:</b> For (c), you may either compute it exactly, or use Hoeffding's inequality to get an approximate solution.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](exercise1.11.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 solution\n",
    "1. **(a)** $S$ cannot produce a hypothesis that is guaranteed to perform better than random on any point outside $D$. If $f$ has 25 $+1$ on $D$ but $-1$ on all other points in $\\mathcal{X}$, $S$ will choose the hypothesis $h_1$, which will not match $f$ outside of $D$ at all. On the other hand, a random function will have $+1$ and $-1$ 50/50, and it matches $f$ half of the time, which is better than the function produced by $S$.\n",
    "\n",
    "2. **(b)** It is possible that $C$ produces a better hypothesis than $S$ produces. See the example above.\n",
    "\n",
    "3. **(c)** If every point in $D$ has 1, then $S$ will choose $h_1$ and $C$ will choose $h_2$. So outside of $D$, $h_1$ will have a 90% chance to match with $f$, while $h_2$ will have only a 10% chance. $S$ will always produce a better hypothesis than $C$.\n",
    "\n",
    "4. **(d)** From the previous problem, we can see that when $p < 0.5$, $C$ will produce a better hypothesis than $S$. Since $C$ always produces $h_2$, which will match $f$ better than $h_1$ if $p < 0.5$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Regularization with Weight decay\n",
    "If we use weight decay regularization ($\\lambda||w||^2)$  for some real number $\\lambda$ in Linear Regression, what \n",
    "happens to the optimal weight vector if we let $\\lambda \\rightarrow \\infty$? (cost is $\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Ex 4: Regularization with Weight Decay\n",
    "\n",
    "We are given the regularized cost function:\n",
    "$$\n",
    "\\text{Cost}(w) = \\frac{1}{n} \\| Xw - y \\|^2 + \\lambda \\| w \\|^2\n",
    "$$\n",
    "To find the optimal weight vector, we minimize the cost function:\n",
    "$$\n",
    "\\left( \\frac{1}{n} X^T X + \\lambda I \\right) w = \\frac{1}{n} X^T y\n",
    "$$\n",
    "Thus, the optimal weight is:\n",
    "$$\n",
    "w = \\left( \\frac{1}{n} X^T X + \\lambda I \\right)^{-1} \\frac{1}{n} X^T y\n",
    "$$\n",
    "As $\\lambda \\to \\infty$, the term $\\lambda I$ dominates, leading to:\n",
    "$$\n",
    "w \\approx \\frac{1}{\\lambda} I \\frac{1}{n} X^T y \\to 0\n",
    "$$\n",
    "\n",
    "### Conclusion:\n",
    "As $\\lambda \\to \\infty$, the weight vector $w \\to 0$, meaning the model becomes fully regularized and ignores the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Grid Search For Regularization and Validation - Sklearn\n",
    "In this exercise we will optimize a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using regularization and validation.\n",
    "You must use the grid search module [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) from sklearn.\n",
    "\n",
    "In the cell below we have shown an example of how to use the grid search module to test two different values for max_depth for a decision tree for wine classification\n",
    "\n",
    "Your job is to find good hyperparameters for decision trees for breast cancer detection.\n",
    "\n",
    "### Task 1:\n",
    "For the breast cancer data set, find the best (or very good) combination of max_depth and min_samples_split (cell two below)\n",
    "\n",
    "The **max_depth** parameter controls the max depth of a tree and the deeper the tree the more complex the model.\n",
    "\n",
    "The **min_samples_split** controls how many elements the algorithm that constructs the tree is allowed to try and split.\n",
    "So if a subtree contains less than min_leaf_size elements, it may not be split into a larger subtree by the algorithm.\n",
    "\n",
    "\n",
    "### Task 2:\n",
    "- How long time does it take to use grid search validation for $k$ hyperparameters where we test each parameter for $d$ values, and the training algorithm uses $f(n)$ time to train on $n$ data points when we split the data into 5 parts.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 30}</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.893126</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.066299</td>\n",
       "      <td>2</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.67977</td>\n",
       "      <td>0.006979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1       0.000675      0.000477         0.000665        0.000470   \n",
       "0       0.001339      0.001256         0.000656        0.000464   \n",
       "\n",
       "   param_max_depth             params  split0_test_score  split1_test_score  \\\n",
       "1               30  {'max_depth': 30}           0.916667           0.830508   \n",
       "0                1   {'max_depth': 1}           0.566667           0.576271   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "1           0.932203         0.893126        0.044729                1   \n",
       "0           0.711864         0.618267        0.066299                2   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "1            1.000000            1.000000            1.000000   \n",
       "0            0.677966            0.672269            0.689076   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "1           1.00000         0.000000  \n",
       "0           0.67977         0.006979  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found {'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "def show_result(clf):\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df = df.sort_values('mean_test_score', ascending=False)\n",
    "    display(df)\n",
    "    print('best parameter found', clf.best_params_)\n",
    "    \n",
    "w_data = load_wine()\n",
    "wine_data = w_data.data\n",
    "wine_labels = w_data.target\n",
    "\n",
    "# grid search validation\n",
    "reg_parameters = {'max_depth': [1, 30]}  # dict with all parameters we need to test\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "clf.fit(wine_data, wine_labels)\n",
    "# code for showing the result\n",
    "bt = show_result(clf)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search took 0.45 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007383</td>\n",
       "      <td>7.963481e-04</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 10}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926161</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.997807</td>\n",
       "      <td>0.992968</td>\n",
       "      <td>0.004484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.007579</td>\n",
       "      <td>4.887142e-04</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 10}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926145</td>\n",
       "      <td>0.016511</td>\n",
       "      <td>2</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.997807</td>\n",
       "      <td>0.992968</td>\n",
       "      <td>0.004484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003978</td>\n",
       "      <td>1.356817e-05</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 5}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924375</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973626</td>\n",
       "      <td>0.975824</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.969298</td>\n",
       "      <td>0.973640</td>\n",
       "      <td>0.003089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004393</td>\n",
       "      <td>4.867086e-04</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 10}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924375</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973626</td>\n",
       "      <td>0.975824</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.975824</td>\n",
       "      <td>0.969298</td>\n",
       "      <td>0.973200</td>\n",
       "      <td>0.002542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006793</td>\n",
       "      <td>1.460389e-03</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 5}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920897</td>\n",
       "      <td>0.022255</td>\n",
       "      <td>5</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.997807</td>\n",
       "      <td>0.996924</td>\n",
       "      <td>0.001759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005794</td>\n",
       "      <td>4.025181e-04</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 10}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920882</td>\n",
       "      <td>0.012649</td>\n",
       "      <td>6</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.980220</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.988136</td>\n",
       "      <td>0.004936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.007283</td>\n",
       "      <td>4.003996e-04</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 5}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917373</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>7</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.997807</td>\n",
       "      <td>0.996924</td>\n",
       "      <td>0.001759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003989</td>\n",
       "      <td>5.917394e-07</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 2}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917358</td>\n",
       "      <td>0.022108</td>\n",
       "      <td>8</td>\n",
       "      <td>0.973626</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.969298</td>\n",
       "      <td>0.974079</td>\n",
       "      <td>0.003498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007380</td>\n",
       "      <td>1.017589e-03</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 2}</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917358</td>\n",
       "      <td>0.021401</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006400</td>\n",
       "      <td>7.897823e-04</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 2}</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915619</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>10</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.991652</td>\n",
       "      <td>0.002915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006183</td>\n",
       "      <td>7.391577e-04</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 5}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915619</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>10</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.989894</td>\n",
       "      <td>0.002244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.006896</td>\n",
       "      <td>7.817854e-04</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 2}</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915588</td>\n",
       "      <td>0.023535</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8        0.007383  7.963481e-04         0.000394        0.000482   \n",
       "11       0.007579  4.887142e-04         0.000599        0.000489   \n",
       "1        0.003978  1.356817e-05         0.000199        0.000399   \n",
       "2        0.004393  4.867086e-04         0.000399        0.000488   \n",
       "7        0.006793  1.460389e-03         0.000592        0.000483   \n",
       "5        0.005794  4.025181e-04         0.000797        0.000399   \n",
       "10       0.007283  4.003996e-04         0.000599        0.000489   \n",
       "0        0.003989  5.917394e-07         0.000808        0.000404   \n",
       "9        0.007380  1.017589e-03         0.000200        0.000400   \n",
       "3        0.006400  7.897823e-04         0.000502        0.000443   \n",
       "4        0.006183  7.391577e-04         0.000399        0.000489   \n",
       "6        0.006896  7.817854e-04         0.000603        0.000493   \n",
       "\n",
       "    param_max_depth  param_min_samples_split  \\\n",
       "8                10                       10   \n",
       "11               15                       10   \n",
       "1                 3                        5   \n",
       "2                 3                       10   \n",
       "7                10                        5   \n",
       "5                 5                       10   \n",
       "10               15                        5   \n",
       "0                 3                        2   \n",
       "9                15                        2   \n",
       "3                 5                        2   \n",
       "4                 5                        5   \n",
       "6                10                        2   \n",
       "\n",
       "                                        params  split0_test_score  \\\n",
       "8   {'max_depth': 10, 'min_samples_split': 10}           0.912281   \n",
       "11  {'max_depth': 15, 'min_samples_split': 10}           0.912281   \n",
       "1     {'max_depth': 3, 'min_samples_split': 5}           0.912281   \n",
       "2    {'max_depth': 3, 'min_samples_split': 10}           0.912281   \n",
       "7    {'max_depth': 10, 'min_samples_split': 5}           0.912281   \n",
       "5    {'max_depth': 5, 'min_samples_split': 10}           0.912281   \n",
       "10   {'max_depth': 15, 'min_samples_split': 5}           0.912281   \n",
       "0     {'max_depth': 3, 'min_samples_split': 2}           0.912281   \n",
       "9    {'max_depth': 15, 'min_samples_split': 2}           0.903509   \n",
       "3     {'max_depth': 5, 'min_samples_split': 2}           0.894737   \n",
       "4     {'max_depth': 5, 'min_samples_split': 5}           0.912281   \n",
       "6    {'max_depth': 10, 'min_samples_split': 2}           0.912281   \n",
       "\n",
       "    split1_test_score  split2_test_score  ...  mean_test_score  \\\n",
       "8            0.938596           0.921053  ...         0.926161   \n",
       "11           0.938596           0.929825  ...         0.926145   \n",
       "1            0.929825           0.938596  ...         0.924375   \n",
       "2            0.929825           0.938596  ...         0.924375   \n",
       "7            0.912281           0.903509  ...         0.920897   \n",
       "5            0.938596           0.921053  ...         0.920882   \n",
       "10           0.903509           0.929825  ...         0.917373   \n",
       "0            0.894737           0.938596  ...         0.917358   \n",
       "9            0.912281           0.921053  ...         0.917358   \n",
       "3            0.938596           0.912281  ...         0.915619   \n",
       "4            0.903509           0.921053  ...         0.915619   \n",
       "6            0.921053           0.903509  ...         0.915588   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "8         0.014408                1            0.995604            0.993407   \n",
       "11        0.016511                2            0.995604            0.993407   \n",
       "1         0.019191                3            0.973626            0.975824   \n",
       "2         0.019191                3            0.973626            0.975824   \n",
       "7         0.022255                5            0.997802            0.993407   \n",
       "5         0.012649                6            0.991209            0.993407   \n",
       "10        0.014424                7            0.997802            0.993407   \n",
       "0         0.022108                8            0.973626            0.978022   \n",
       "9         0.021401                8            1.000000            1.000000   \n",
       "3         0.016401               10            0.993407            0.995604   \n",
       "4         0.013292               10            0.993407            0.989011   \n",
       "6         0.023535               12            1.000000            1.000000   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "8             0.993407            0.984615            0.997807   \n",
       "11            0.993407            0.984615            0.997807   \n",
       "1             0.971429            0.978022            0.969298   \n",
       "2             0.971429            0.975824            0.969298   \n",
       "7             0.997802            0.997802            0.997807   \n",
       "5             0.984615            0.980220            0.991228   \n",
       "10            0.997802            0.997802            0.997807   \n",
       "0             0.971429            0.978022            0.969298   \n",
       "9             1.000000            1.000000            1.000000   \n",
       "3             0.991209            0.986813            0.991228   \n",
       "4             0.989011            0.986813            0.991228   \n",
       "6             1.000000            1.000000            1.000000   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "8           0.992968         0.004484  \n",
       "11          0.992968         0.004484  \n",
       "1           0.973640         0.003089  \n",
       "2           0.973200         0.002542  \n",
       "7           0.996924         0.001759  \n",
       "5           0.988136         0.004936  \n",
       "10          0.996924         0.001759  \n",
       "0           0.974079         0.003498  \n",
       "9           1.000000         0.000000  \n",
       "3           0.991652         0.002915  \n",
       "4           0.989894         0.002244  \n",
       "6           1.000000         0.000000  \n",
       "\n",
       "[12 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found {'max_depth': 10, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "cancer_data = load_breast_cancer()\n",
    "c_data = cancer_data.data\n",
    "c_labels = cancer_data.target\n",
    "\n",
    "\n",
    "def decisiontree_model_selection(train_data, labels):\n",
    "    clf = None\n",
    "    ###YOUR CODE HERE\n",
    "    # Set up the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    #Initialize the DecisionTreeClassifier\n",
    "    dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "    #Set up the GridSearchCV\n",
    "    clf = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, return_train_score=True)\n",
    "\n",
    "    #Measure the time taken for grid search\n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "    #Fit the model to the training data\n",
    "    clf.fit(train_data, labels)\n",
    "\n",
    "    end_time = time.time()  #End the timer\n",
    "\n",
    "    #Calculate the total time taken\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Grid search took {total_time:.2f} seconds.\")\n",
    "\n",
    "    #Return the classifier\n",
    "    return clf\n",
    "    ### END CODE\n",
    "    return clf\n",
    "###\n",
    "clf = decisiontree_model_selection(c_data, c_labels)\n",
    "bt = show_result(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6: VC Dimension of Hyperplanes (Book Exercise 2.4 p. 52)\n",
    "Consider the input space $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$ (with the first coordinate being the constant 1). Show that the VC dimension of the hypothesis space $\\mathcal{H} = \\{\\textrm{sign}(w^\\intercal x) \\mid w\\in \\mathbb{R}^{d+1} \\}$ corresponding to the perceptron is $d+1$.\n",
    "\n",
    "We need to show \n",
    "1. That there exists a data set of size d+1 that can be shattered by hyperplanes\n",
    "2. That no data set of size d+2 can be shattered by hyperplanes\n",
    "\n",
    "We will give a few more hints than the book does.\n",
    "### Shattering d+1 points\n",
    "As the book hints you must create an \"easy\" data set that you store in matrix $X$. \n",
    "\n",
    "**Hint:** We suggest you consider as a data matrix, the $(d+1) \\times (d+1)$ matrix $X$ whose first column is all-1s (required since $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$) and where the lower $d \\times d$ corner of the matrix is the $d \\times d$ identity matrix.\n",
    "\n",
    "Show that you can construct any dichotomy $y \\in \\{-1,+1\\}^{d+1}$ using some $h \\in \\mathcal{H}$ and the data matrix $X$ defined above. That is, you have to show that for any $y \\in \\{-1,+1\\}^{d+1}$, you can find some hypothesis $w$ such that for all $i$, we have $\\textrm{sign}(w^\\intercal x_i)=y_i$ where $x_i$ is the $i$'th row of $X$.\n",
    "\n",
    "\n",
    "### No Shattering of d+2 points.\n",
    "Must show that for any d+2 points, there is a  dichotomy hyperplanes can not capture.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Consider an arbitrary set of d+2 points $x_1,\\dots, x_{d+2}$ of dimension (d+1) and think of them as vectors in $\\{1\\} \\times \\mathbb{R}^d \\subset \\mathbb{R}^{d+1}$.\n",
    "Since we have more vectors than dimensions the vectors must be linearly dependent.\n",
    "\n",
    "i.e. there is a $j$ such that:\n",
    "$$\n",
    "x_j = \\sum_{i\\neq j} a_i x_i\n",
    "$$\n",
    "Since $x_j$ is determined by the other data points then so is $w^\\intercal x_j$ for any $w$. This means the classification on point $x_j$ is dictated by the classification of the other data points and thus cannot freely be chosen.\n",
    "i.e.\n",
    "$$\n",
    "w^\\intercal x_j = w^\\intercal \\sum_{i\\neq j} a_i x_i =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Define an impossible dichotomy as follows. \n",
    "$$\n",
    "y_i = \\textrm{sign}(a_i), \\quad i\\neq j, \\quad y_j = -1\n",
    "$$\n",
    "Show this dichotomy is impossible!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](exercise2.4.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Exercise If Time 7: Book Problem 2.18 In short\n",
    "Define\n",
    "$$\n",
    "\\mathcal{H}= \\{h_\\alpha \\mid h_\\alpha(x) = (-1)^{\\lfloor \\alpha\n",
    "          x\\rfloor}, \\alpha \\in {{\\mathbb R}}\\}\n",
    "$$ \n",
    "\n",
    "Show that the VC dimension of ${{\\mathcal H}}$ is infinite (even though there is only one parameter!)\n",
    "\n",
    "Hint: Use the points set\n",
    "$x_1=10,x_2=100,\\dots,x_i = 10^i,\\dots,x_N=10^N$ and show how to implement any dichotomy $y_1,\\dots,y_N \\in \\{-1, +1\\}^N$ (find $\\alpha$ that works).\n",
    "You can safely assume $\\alpha >0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
