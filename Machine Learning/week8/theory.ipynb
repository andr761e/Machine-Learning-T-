{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: AdaBoost and VC-dimension\n",
    "In this exericse, we will investigate the VC-dimension of voting classifiers and argue that one has to be careful with the number of iterations of boosting to guarantee good generalization performance with high probability.\n",
    "\n",
    "The data we will consider is one-dimensional, i.e. a single feature. Our weak learner is decision stumps, i.e. a base hypothesis $h$ can be specified by a splitting value $v$, and two leaf values $a, b \\in \\{-1,1\\}$. Given a one-dimensional feature vector $x = (x_1)$, the hypothesis $h$ returns $a$ when $x_1 \\leq v$ and it returns $b$ otherwise. We let $H$ denote the set of all such decision stumps, i.e. $H$ is the set of base hypotheses.\n",
    "\n",
    "### VC-dimension of base classifier\n",
    "1. Show that the VC-dimension of $H$ is $2$.\n",
    "\n",
    "### VC-dimension of voting classifiers\n",
    "Let $C$ be the class of all voting classifiers over $H$, i.e. $C$ consists of all hypotheses of the form:\n",
    "$$\n",
    "f(x) = \\textrm{sign}(\\sum_{h \\in H} \\alpha_h h(x))\n",
    "$$\n",
    "We will show that $C$ has infinite VC-dimension. To do so, we will argue that for any size $n$, there is a set of $n$ points $x_1,\\dots,x_n$ in 1d that can be shattered, i.e. for any labeling $y \\in \\{-1,1\\}^n$ of the $n$ points, there is a hypothesis $g$ in $C$ such that $g(x_i)=y_i$ for all $i$. \n",
    "\n",
    "2. First argue that for any interval $[a, b]$ and value $z \\in \\{-1,1\\}$, it is possible to find decision stumps $h_1,h_2$ such that for $x \\in (a, b]$, we have $h_1(x)+h_2(x) = 2z$ and for any $x \\notin (a, b]$, we have $h_1(x) + h_2(x) = 0$.\n",
    "\n",
    "3. Use what you showed in 2. to come up with a 1d data set of size $n$ that you can shatter, i.e. generate any possible dichotomy/labeling using a voting classifier from $C$.\n",
    "\n",
    "### Growth function for few base learners\n",
    "Finally, consider if we restrict ourselves to only using voting classifiers that combine the predictions of no more than $T$ base hypotheses. To simplify the exercise, we will even assume that we use precisely the coefficients $\\alpha_h=1$ for the $T$ hypotheses. That is, we consider the set of voting classifiers $C^T$ containing all hypotheses of the form:\n",
    "$$\n",
    "f(x) = \\textrm{sign}(\\sum_{i=1}^T h_i(x))\n",
    "$$\n",
    "where $h_1,\\dots,h_T$ are hypotheses from $H$.\n",
    "\n",
    "Recall that we have generalization bounds in terms of the growth function $m_H(n)$:\n",
    "$$\n",
    "\\Pr[|E_{out}(h^*)-E_{in}(h^*)| > \\varepsilon] < 8m_H(2n)e^{-\\varepsilon^2 n/8}.\n",
    "$$\n",
    "\n",
    "4. What is the growth function $m_H(n)$ when $H$ is the set of decision stumps on 1d data?\n",
    "\n",
    "5. What is a good upper bound on the growth function $m_{C^T}(n)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 solution\n",
    "We are tasked with showing that the VC-dimension of the set of decision stumps $H$ is 2. \n",
    "\n",
    "#### Definition of VC-dimension\n",
    "\n",
    "The **VC-dimension** of a hypothesis class is the largest number of points that can be shattered by the class. A set of points is **shattered** if, for every possible labeling of the points, there exists a hypothesis in the class that correctly classifies the points according to that labeling.\n",
    "\n",
    "#### Step 1: Shattering Two Points\n",
    "\n",
    "Consider two points $x_1$ and $x_2$ on the real line. There are four possible labelings for these two points:\n",
    "\n",
    "1. $ (+1, +1) $\n",
    "2. $ (+1, -1) $\n",
    "3. $ (-1, +1) $\n",
    "4. $ (-1, -1) $\n",
    "\n",
    "We will now demonstrate that each of these labelings can be correctly classified by some decision stump:\n",
    "\n",
    "- **Labeling $ (+1, +1) $**: Set the threshold $v$ to be greater than both $x_1$ and $x_2$, classifying both points as +1.\n",
    "- **Labeling $ (+1, -1) $**: Set the threshold $v$ between $x_1$ and $x_2$, such that $x_1$ is classified as +1 and $x_2$ as -1.\n",
    "- **Labeling $ (-1, +1) $**: Similarly, set the threshold $v$ between $x_1$ and $x_2$ such that $x_1$ is classified as -1 and $x_2$ as +1.\n",
    "- **Labeling $ (-1, -1) $**: Set the threshold $v$ to be less than both $x_1$ and $x_2$, classifying both points as -1.\n",
    "\n",
    "Since we can correctly classify all four possible labelings of $x_1$ and $x_2$, the two points can be **shattered** by the set of decision stumps.\n",
    "\n",
    "#### Step 2: Failing to Shatter Three Points\n",
    "\n",
    "Now, consider three points $x_1$, $x_2$, and $x_3$ on the real line. There are eight possible labelings for these points, but we will show that they cannot all be correctly classified by decision stumps. \n",
    "\n",
    "For example, consider the labeling $ (+1, -1, +1) $. To correctly classify this set of points, we would need two thresholds:\n",
    "- One threshold to separate $x_1$ (labeled +1) from $x_2$ (labeled -1).\n",
    "- Another threshold to separate $x_2$ (labeled -1) from $x_3$ (labeled +1).\n",
    "\n",
    "However, a decision stump can only have one threshold, so it cannot correctly classify this set of points. Therefore, three points cannot be shattered by decision stumps.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Since two points can be shattered by decision stumps, but three points cannot, the **VC-dimension** of the hypothesis class $H$ (the set of decision stumps) is:\n",
    "\n",
    "$$\n",
    "\\boxed{2}\n",
    "$$\n",
    "\n",
    "---\n",
    "### Question 2: Argue for any interval $[a, b]$ and value $z \\in \\{-1, 1\\}$, it is possible to find decision stumps $h_1$, $h_2$ such that for $x \\in (a, b]$, we have $h_1(x) + h_2(x) = 2z$ and for $x \\notin (a, b]$, we have $h_1(x) + h_2(x) = 0$.\n",
    "\n",
    "#### Solution:\n",
    "The goal is to construct decision stumps $h_1$ and $h_2$ to satisfy the given conditions inside and outside the interval $(a, b]$. Decision stumps classify points based on whether they are below or above a threshold, making them piecewise constant functions.\n",
    "\n",
    "We can define the two decision stumps as follows:\n",
    "- Let $h_1(x)$ be a stump that returns $z$ for all $x > a$ and returns 0 otherwise.\n",
    "- Let $h_2(x)$ be a stump that returns $z$ for all $x \\leq b$ and returns 0 otherwise.\n",
    "\n",
    "Together, these two stumps satisfy the condition:\n",
    "- For $x \\in (a, b]$, both $h_1(x) = z$ and $h_2(x) = z$, so $h_1(x) + h_2(x) = 2z$.\n",
    "- For $x \\notin (a, b]$, at least one of the stumps returns 0, so $h_1(x) + h_2(x) = 0$.\n",
    "\n",
    "Thus, we have constructed the required decision stumps.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3: Use what you showed in 2. to come up with a 1D data set of size $n$ that you can shatter, i.e., generate any possible dichotomy/labeling using a voting classifier from $\\mathcal{C}$.\n",
    "\n",
    "#### Solution:\n",
    "From Question 2, we now know that using two decision stumps, we can create functions that behave like an indicator function for any interval $(a, b]$. This means that by combining multiple pairs of decision stumps, we can shatter any set of points in 1D.\n",
    "\n",
    "For a data set of size $n$, we can use $n$ intervals. Each interval can be treated as having two decision stumps $h_1, h_2$ such that:\n",
    "- For any point $x_i$, we can find two decision stumps that create a function that assigns $+1$ or $-1$ to that point.\n",
    "- Thus, we can assign any possible labeling to the points by creating a voting classifier that is a linear combination of these decision stumps.\n",
    "\n",
    "Since we can generate any possible dichotomy for any set of points, we can shatter a set of size $n$ using voting classifiers from $\\mathcal{C}$.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4: What is the growth function $m_H(n)$ when $H$ is the set of decision stumps on 1D data?\n",
    "\n",
    "#### Solution:\n",
    "The growth function $m_H(n)$ represents the maximum number of distinct labelings that can be generated by a hypothesis class $H$ for a sample of size $n$.\n",
    "\n",
    "For decision stumps on 1D data, the VC-dimension is 2 (as shown in Question 1), meaning:\n",
    "- For any set of 2 points, we can realize all 4 possible labelings.\n",
    "- For any set of 3 or more points, we cannot realize all possible labelings.\n",
    "\n",
    "Thus, the growth function $m_H(n)$ for decision stumps is:\n",
    "- For $n \\leq 2$, $m_H(n) = 2^n$ (since we can shatter sets of size up to 2).\n",
    "- For $n > 2$, the growth function follows the form of a polynomial bound based on the VC-dimension, i.e., $m_H(n) = \\binom{n}{\\leq 2} = n^2$.\n",
    "\n",
    "In summary:\n",
    "$$\n",
    "m_H(n) =\n",
    "\\begin{cases}\n",
    "2^n & \\text{for } n \\leq 2 \\\\\n",
    "n^2 & \\text{for } n > 2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5: What is a good upper bound on the growth function $m_{\\mathcal{C}^T}(n)$?\n",
    "\n",
    "#### Solution:\n",
    "The class $\\mathcal{C}^T$ contains voting classifiers that combine no more than $T$ base hypotheses (decision stumps). Each classifier in $\\mathcal{C}^T$ can be represented as:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}\\left(\\sum_{i=1}^T h_i(x)\\right)\n",
    "$$\n",
    "\n",
    "Given that there are at most $T$ base hypotheses, we need to consider the possible combinations of those hypotheses. The growth function for this class can be bounded by:\n",
    "\n",
    "$$\n",
    "m_{\\mathcal{C}^T}(n) \\leq \\sum_{k=0}^T \\binom{n}{k}\n",
    "$$\n",
    "\n",
    "This sum gives the number of ways to select and combine up to $T$ base hypotheses to form a voting classifier. For small $T$, this results in a polynomial bound on the growth function. Thus, the growth function grows at most as fast as $O(n^T)$.\n",
    "\n",
    "Therefore, a good upper bound for the growth function $m_{\\mathcal{C}^T}(n)$ is:\n",
    "\n",
    "$$\n",
    "m_{\\mathcal{C}^T}(n) \\leq n^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implementing AdaBoost\n",
    "In this exercise your task is to implement AdaBoost as described in the lecture and the Boosting note.\n",
    "We have provided starter code in adaboost.py. See the boosting note for a description of the algorithm.\n",
    "\n",
    "You must implement the methods\n",
    "- ensemble_output\n",
    "- exp_loss\n",
    "- predict\n",
    "- score\n",
    "- fit\n",
    "\n",
    "in that order\n",
    "\n",
    "To test your implementation, run adaboost.py\n",
    "\n",
    "You should get a final accuracy of around 0.886\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradient Boosting by Hand\n",
    "In this exercise you must complete one step of gradient boosting with exponential loss on a small 1d data set (X, y) as shown below. The exponential loss is $L(h(x),y) = e^{-yh(x)}$.\n",
    "\n",
    "$X = [1,2,5,3,4]^\\intercal$\n",
    "\n",
    "$y = [1,1,1,-1, -1]^\\intercal$\n",
    "\n",
    "Assume that we initialize our ensemble model with the constant function $h(x) = 1$\n",
    "\n",
    "\n",
    "**Your task requires the following three steps** \n",
    "1. Compute the \"gradient\" vector $\\hat{y}$ (as in the slides) that we need to fit.\n",
    "2. Construct the best Regression Stump for fitting $\\hat{y}$.\n",
    "3. Optimize the leaf values such that the newly added tree minimize the exponential loss, with the condition that the value the leaf returns is in the interval [-1, 1].\n",
    "   What happens if we do not have this constraint that the leaf must return values in [-1, 1]?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Gradient Boosting by Hand\n",
    "Given the dataset:\n",
    "\n",
    "$$\n",
    "X = [1, 2, 5, 3, 4]^\\top\n",
    "$$\n",
    "$$\n",
    "y = [1, 1, 1, -1, -1]^\\top\n",
    "$$\n",
    "\n",
    "We are tasked with performing one step of gradient boosting using exponential loss:\n",
    "\n",
    "$$\n",
    "L(h(x), y) = e^{-y h(x)}\n",
    "$$\n",
    "\n",
    "The initial ensemble model is a constant function $h(x) = 1$.\n",
    "\n",
    "#### Step 1: Compute the \"gradient\" vector $\\hat{y}$\n",
    "\n",
    "The gradient of the exponential loss function with respect to $h(x)$ is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = y e^{-y h(x)}\n",
    "$$\n",
    "\n",
    "Since $h(x) = 1$, we compute the gradient for each data point:\n",
    "\n",
    "For $y = 1$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_1 = e^{-1}, \\quad \\hat{y}_2 = e^{-1}, \\quad \\hat{y}_3 = e^{-1}\n",
    "$$\n",
    "\n",
    "For $y = -1$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_4 = -e, \\quad \\hat{y}_5 = -e\n",
    "$$\n",
    "\n",
    "Thus, the gradient vector is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = [e^{-1}, e^{-1}, e^{-1}, -e, -e]^\\top\n",
    "$$\n",
    "\n",
    "#### Step 2: Construct the best regression stump for fitting $\\hat{y}$\n",
    "\n",
    "To find the best regression stump, we need to split the data and assign constant values to each region. The goal is to minimize the squared error between the predicted values of the stump and the actual gradient values $\\hat{y}$.\n",
    "\n",
    "1. Try different split points between the values of $X$.\n",
    "2. For each split, calculate the average of $\\hat{y}$ in the two regions and compute the squared error.\n",
    "3. Choose the split that minimizes the squared error.\n",
    "\n",
    "#### Step 3: Optimize the leaf values to minimize exponential loss\n",
    "\n",
    "The goal is to find the leaf values $v$ that minimize the exponential loss:\n",
    "\n",
    "$$\n",
    "L(v) = \\sum_{i \\in \\text{leaf}} e^{-y_i \\cdot (h(x_i) + v)}\n",
    "$$\n",
    "\n",
    "To minimize the exponential loss for each leaf, the optimal $v^*$ is given by:\n",
    "\n",
    "$$\n",
    "v^* = \\frac{\\sum_{i \\in \\text{leaf}} y_i}{\\sum_{i \\in \\text{leaf}} 1}\n",
    "$$\n",
    "\n",
    "This is the average label in each region. If the values are constrained to lie in $[-1, 1]$, we clip the values such that $v^* \\in [-1, 1]$.\n",
    "\n",
    "If there is no constraint, the optimal $v^*$ can be any real number that minimizes the loss.\n",
    "\n",
    "---\n",
    "\n",
    "By following these steps, you can complete one iteration of gradient boosting on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implementing Gradient Boosting\n",
    "In this exercise your task is to implement the Gradient Boosting algorithm for regression using Least Squares Loss.\n",
    "We have provided starter code in **gradientboost.py**. \n",
    "\n",
    "You must implement the methods\n",
    "- predict\n",
    "- score\n",
    "- fit\n",
    "\n",
    "in that order.\n",
    "\n",
    "**Note**: To simplify the implementation, we will **not** require that the first hypothesis you train is a constant function. Instead, just train a regression tree and give it an $\\alpha$ of $1$.\n",
    "\n",
    "**Note**: For least squares loss, gradient boosting is MUCH simpler to implement. Concretely, in any iteration, once we have computed the best regression tree using least squares loss, gradient boosting would alter the leaf values to minimize the loss. However, for least squares loss, it is still the mean we would return. Hence we do not need to alter leaf return values at all!\n",
    "\n",
    "Notice that fit gets two sets of data and labels X, y and X_val, y_val.\n",
    "The latter X_val, y_val is a separate validation test set you must test your current ensemble on in each iteration so we can plot the development on data not used for training.\n",
    "\n",
    "To test your implementation, run gradientboost.py -max_depth 1\n",
    "\n",
    "You can provide different max_depth of the base learner which is a Regression Tree (1 is default).\n",
    "\n",
    "With a default base learner with max depth 1 the mean least squares error on both training and test data should be around 0.35. \n",
    "If you change random state then the results may be different.\n",
    "\n",
    "If you increase the max_depth the results will change.  Try for instance max_depth 3 and 5 as well. What do you see?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
