{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Exercises - Week 2\n",
    "**Like last week, it is very imporant that you try to solve every exercise. It is not important that you answer correctly. Spend no more than 5-10 min on each exercise. If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n",
    "\n",
    "The TA's will be very happy to answer questions during the TA session or on the board.\n",
    "\n",
    "Do not despair if you cannot solve them, but try to understand the question and pinpoint which parts you do not understand. \n",
    "\n",
    "# 1. Learning Types\n",
    "In this exercise you must distinguish between Supervised Learning and Unsupervised Learning. Imagine you work at a company that sells *stuff*. The company stores information about its costumers. For each costumer the company saves the following 5 attributes:\n",
    "\n",
    "    AGE, SEX, INCOME, RESIDENCE, MONEY USED AT COMPANY\n",
    "\n",
    "<b>Question 1: </b><br>In each of the following examples you should determine if the problem is a Supervised or Unsupervised learning problem.\n",
    "\n",
    "-   The company wants to learn how to predict 'MONEY USED AT COMPANY' given 'AGE', 'SEX', 'INCOME' and 'RESIDENCE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to learn ways of grouping costumers depending on 'AGE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to learn how to predict 'SEX' given 'MONEY SPENT AT COMPANY' and 'AGE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to target different groups of costumers depending on 'AGE', 'INCOME' and 'MONEY SPENT AT COMPANY'. Supervised or Unsupervised?   \n",
    "\n",
    "<br><br>\n",
    "<b>Question 2</b>:<br>\n",
    "In supervised learning the data is of the form $D_{supervised}=\\{(x_1,y_1),...,(x_n,y_n)\\}$. <br>\n",
    "In unsupervised learning we have data of the form $D_{unsupervised}=\\{x_1,...,x_n\\}$.\n",
    "\n",
    "Write the form the data would take in each case from Question 1.\n",
    "\n",
    "HINT: Possible solutions to two of the cases\n",
    "\n",
    "$$D=\\{20\\text{ years},\\;21\\text{ years}, \\;23\\text{ years}, ... \\}$$ \n",
    "$$D=\\{([100\\text{ kr},\\;22\\text{ years}],\\text{ male}),\\;([120\\text{ kr},\\;30\\text{ years}],\\text{ female}), ...\\}$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 answers \n",
    "### Question 1\n",
    "First is supervised because they give them an output of much person spend which they can use to predict future people's spending based on the same parameters. \n",
    "\n",
    "Second is unsupervised. They give input without labels and the machine has to come up with a way of grouping them. \n",
    "\n",
    "Third is likely supervised as well. We first give them input containing labels with gender which it uses to predict gender.  \n",
    "\n",
    "Fourth is likely unsupervised as we want the machiene to divide our dat into the different groups we should target. \n",
    "\n",
    "\n",
    "### Question 2\n",
    "So each data point is a tuple that contains an $x_i$ and an $y_i$. \n",
    "#### example 1:\n",
    "$$\n",
    "D_{supervised} = \\{([30 years, male, 50000 kr,Aaarhus],1500 kr), \\dots\\}\n",
    "$$\n",
    "$$\n",
    "D_{supervised} = \\{([int, boolean, int, string],int), \\dots\\}\n",
    "$$\n",
    "#### example 2:\n",
    "$$\n",
    "D_{unsupervised} = \\{[30 years], [23 years], \\dots\\}\n",
    "$$\n",
    "$$\n",
    "D_{unsupervised} = \\{[int],[int], \\dots \\}\n",
    "$$\n",
    "#### example 3:\n",
    "$$\n",
    "D_{supervised} = \\{([100\\text{ kr},\\;22\\text{ years}],\\text{ male}),\\;([120\\text{ kr},\\;30\\text{ years}],\\text{ female}), ...\\}\n",
    "$$\n",
    "$$\n",
    "D_{supervised} = \\{([int, int],boolean), \\dots\\}\n",
    "$$\n",
    "#### example 4:\n",
    "$$\n",
    "D_{unsupervised} = \\{[23 years, 50000 kr, 500kr], \\dots\\}\n",
    "$$\n",
    "$$\n",
    "D_{unsupervised} = \\{[int, int,int], \\dots \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression Or Classification\n",
    "\n",
    "<b>Question 1</b>: <br>In each of the following examples you should distinguish between regression and classification.\n",
    "\n",
    "-   In the previous question the company wanted to predict 'MONEY SPENT AT COMPANY' from ('AGE', 'SEX', 'INCOME', 'RESIDENCE'). Is that regression or classification?\n",
    "\n",
    "-   Recognizing the color of wine as white, rose or red. Is that regression or classification?\n",
    "\n",
    "-   Predicting a students grade in machine learning as a function of previous grades (on the 12 scale). Is that regression or classification? \n",
    "    \n",
    "-   Predicting email as spam, normal. Regression or classification?\n",
    "<br><br>\n",
    "    \n",
    "<b>Question 2: </b> <br>\n",
    "In supervised learning we want to approximate an unkown target function $f:X\\rightarrow Y$. In regression we could have $Y=\\mathbb{R}$ and in classification we could have $Y=\\{c_1,...,c_k\\}$.\n",
    "\n",
    "What is $Y$ in the above four cases? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 answers \n",
    "### Question 1\n",
    "First is likely regression. Regression is used when the target variable (output) is continuous. Here, \"MONEY SPENT AT COMPANY\" is likely a continuous variable, meaning you're predicting a numerical value\n",
    "\n",
    "Second is defintley classification since the target variable is categorical. \n",
    "\n",
    "Third is also likely classification as it is a numerical value, but we have to find out which group of number he should fit into. \n",
    "\n",
    "Fourth is classification since the output is of categorical boolean nature: Output is either spam of not spam. \n",
    "\n",
    "### Question 2\n",
    "#### Case 1: \n",
    "$$\n",
    "Y = \\mathbb{R}\n",
    "$$\n",
    "#### Case 2: \n",
    "$$\n",
    "Y = \\{white,rose,red\\}\n",
    "$$\n",
    "#### Case 3: \n",
    "$$\n",
    "Y = \\{-3,0,02,4,7,10,12\\}\n",
    "$$\n",
    "#### Case 4: \n",
    "$$\n",
    "Y = \\{spam,normal\\}\n",
    "$$\n",
    "Last one could also just be output 1 or 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Perceptron\n",
    "### Question 1: Running the Perceptron Learning Algorithm\n",
    "Assume we are given a training data set with 3 features, of which the first is hardcoded to 1. The data consists of the four examples $((1,2,2), 1), ((1, 2, 3), 1), ((1, 4, 2), -1), ((1, 4, 0), -1)$. What hypothesis $w = (w_0,w_1,w_2)$ does it return when initialized with $w = (0,0,0)$ and where we always pick the first misclassified point when updating? NOTE: We assume $sign(0)=0$ and thus is different from all labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [1 2 2]\n",
      "Updated weights: [ 0 -2  0]\n",
      "Updated weights: [1 0 2]\n",
      "Updated weights: [ 0 -4  0]\n",
      "Updated weights: [ 1 -2  2]\n",
      "Final weights: [ 1 -2  2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Define the data points and labels\n",
    "data = np.array([\n",
    "    [1, 2, 2],\n",
    "    [1, 2, 3],\n",
    "    [1, 4, 2],\n",
    "    [1, 4, 0]\n",
    "])\n",
    "\n",
    "labels = np.array([1, 1, -1, -1])\n",
    "\n",
    "#Initialize weights\n",
    "w = np.array([0, 0, 0])\n",
    "\n",
    "#Perceptron Learning Algorithm\n",
    "def perceptron(data, labels, w, max_iterations=10):\n",
    "    for _ in range(max_iterations):\n",
    "        for i in range(len(data)):\n",
    "            if np.sign(np.dot(w, data[i])) != labels[i]:\n",
    "                w = w + labels[i] * data[i]\n",
    "                print(f\"Updated weights: {w}\")\n",
    "    return w\n",
    "\n",
    "#Run the Perceptron Algorithm\n",
    "final_weights = perceptron(data, labels, w)\n",
    "print(f\"Final weights: {final_weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Choosing leaf return value in Decision Trees \n",
    "## (Problem 1.12 from 'Learning From Data')\n",
    "\n",
    "Given $y_1\\leq\\cdots\\leq y_n\\in \\mathbb{R}$ find $h\\in \\mathbb{R}$ that on average is closest to $y_1,\\ldots,y_n$ measured by squared distance (least squares). That is, \n",
    "$$\n",
    "h_\\textrm{mean} =\\textrm{arg}\\min_h \\sum_{i=1}^n (h-y_i)^2\n",
    "$$ \n",
    "<b>Question 1: </b>Show that\n",
    "$h_\\textrm{mean} = \\frac{1}{n} \\sum_{i=1}^n y_i$ is the minimizer. \n",
    "\n",
    "HINT: Computing the <a href=\"https://en.wikipedia.org/wiki/Chain_rule\">derivative</a> may be worth the time and strain on your brain.\n",
    "\n",
    "HINT: a local minimum is a global minimum!\n",
    "\n",
    "\n",
    "<b>Question 2: </b>Consider absolute deviation instead of squared distance, i.e.\n",
    "\n",
    "$$h_\\mathrm{med} =\\textrm{arg}\\min_h \\sum_{i=1}^n |h-y_i|$$ \n",
    "\n",
    "Show that $h_\\mathrm{med} = \\mathrm{median}(y_1,\\dots,y_n)$, the median of the $y$ values is the minimizer. \n",
    "\n",
    "HINT: Computing derivative may be usefull but  $|a|$ is not differentiable at zero but you may set it to zero (ask google about subgradients if you are interested).  \n",
    "\n",
    "HINT: You can also argue purely algorithmically by thinking about what happens with the cost as we sweep *h* from $-\\infty$ to $\\infty$).\n",
    "\n",
    "HINT: a local minimum is a global minimum!\n",
    "\n",
    "<b>Question 3: </b>What happens to the solutions $h_\\mathrm{mean}, h_\\mathrm{med}$ if we\n",
    "add noise the last element $y_n$, i.e. $y_n = y_n + \\varepsilon$ for\n",
    "$\\varepsilon \\rightarrow \\infty$.\n",
    "\n",
    "\n",
    "\n",
    "Which method is more stable for outliers (data that looks nothing like the remaining data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 solution\n",
    "### question 1\n",
    "We have: \n",
    "$$\n",
    "E_{in}(h) = \\sum_{i=1}^{N} (h-y_i)^2\n",
    "$$\n",
    "We start by taking the derivative: \n",
    "$$\n",
    "\\frac{\\partial E_{in}(h)}{\\partial h} = \\sum_{i=1}^{N} 2(h-y_i) = 2 \\sum_{i=1}^{N} (h-y_i)\n",
    "$$\n",
    "We set this derivative to 0 and solve for $h$, which would be the mean: \n",
    "$$\n",
    "2 \\sum_{i=1}^{N} (h-y_i) = 0 \n",
    "$$\n",
    "Deler med 2 på begge sider: \n",
    "$$\n",
    "\\sum_{i=1}^{N} (h-y_i) = 0 \n",
    "$$\n",
    "Udvider summen: \n",
    "$$\n",
    "\\sum_{i=i}^{N} h - \\sum_{i=1}^{N} y_i = 0 \n",
    "$$\n",
    "Løser første sum\n",
    "$$\n",
    "N \\cdot h - \\sum_{i=1}^{N} y_i = 0 \n",
    "$$\n",
    "Isolerer summen\n",
    "$$\n",
    "N \\cdot h = \\sum_{i=1}^{N} y_i \n",
    "$$\n",
    "Deler med $N$: \n",
    "$$\n",
    "h_{mean} = \\frac{1}{N} \\sum_{i=1}^{N} y_i \n",
    "$$\n",
    "\n",
    "### question 2\n",
    "We now have: \n",
    "$$\n",
    "E_{in}(h) = \\sum_{i=1}^{N} |h-y_i|\n",
    "$$\n",
    "We start by taking the derivative: \n",
    "$$\n",
    "\\frac{\\partial E_{in}(h)}{\\partial h} = \\sum_{i=1}^{N} sign(h - y_i)\n",
    "$$\n",
    "The sign function basically gives us output +1 if $h > y_i$, output -1 if $h < y_i$ and output 0 if $h = y_i$. To find the median we set the derivative to 0: \n",
    "$$\n",
    "\\sum_{i=1}^{N} sign(h - y_i) = 0\n",
    "$$\n",
    "Since the sum is equal to 0, it basically tells us that half the points are bigger than $h$ and half the points are lower than $h$ which is how they got the median. \n",
    "\n",
    "### question 3\n",
    "If $y_N$ is perturbed to $y_N + \\epsilon$, where $\\epsilon \\rightarrow \\infty$, we can see that $h_{mean}$ will increase a lot since $y_N$ contributes to its calculation, but $h_{med}$ can stay the same, because it only requires $h_{med} < y_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Decision Tree Cost with Entropy\n",
    "In this exercise we examine the entropy-based approach to constructing decision stumps. Recall that, for any leaf $\\ell$, the entropy in that leaf is $H(\\ell):=-\\sum_{i=0}^{k-1} p_i \\lg_2(p_i)$. Here $p_i$ denotes the fraction of training examples in that leaf having the label $i$. For binary classification, we thus have $k=2$.\n",
    "\n",
    "The entropy of the entire tree $T$ is $H(T):=\\sum_{\\ell} (n_\\ell/n) H(\\ell)$, where $\\ell$ sums over all leaves and $n_\\ell$ is the number of training examples in leaf $\\ell$.\n",
    "\n",
    "We consider classification into the $k=3$ classes Red (0), White (1), Rose (2). We have $n=9$ training examples. The data has just one feature. The data and labels are as follows:\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "  9. \\\\\n",
    "  33. \\\\\n",
    "  20. \\\\\n",
    "  27. \\\\\n",
    "  3. \\\\\n",
    "  6. \\\\\n",
    "  18. \\\\\n",
    "  14. \\\\\n",
    "  16. \\\\\n",
    "  \\end{bmatrix}\n",
    ", \\quad\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  0 \\\\ \n",
    "  1 \\\\\n",
    "  1 \\\\\n",
    "  1 \\\\\n",
    "  2 \\\\\n",
    "  2 \\\\\n",
    "\\end{bmatrix} $$\n",
    "  \n",
    "We consider the split $x < 19$. \n",
    "\n",
    "**Task:** \n",
    "  - Compute the entropy of the left and right leaf using this split.\n",
    "  - Compute the entropy of the full tree using this split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 solution\n",
    "#### Left leaf entropy\n",
    "We split based on the condition $x < 19$ which means that in the left leaf we have data: \n",
    "$$ X_l = \\begin{bmatrix}\n",
    "  9. \\\\\n",
    "  3. \\\\\n",
    "  6. \\\\\n",
    "  18. \\\\\n",
    "  14. \\\\\n",
    "  16. \\\\\n",
    "  \\end{bmatrix}\n",
    ", \\quad\n",
    "Y_l = \n",
    "\\begin{bmatrix}\n",
    "  0 \\\\ \n",
    "  1 \\\\\n",
    "  1 \\\\\n",
    "  1 \\\\\n",
    "  2 \\\\\n",
    "  2 \\\\\n",
    "\\end{bmatrix} $$\n",
    "We first consider the distribution of classes in vector $Y_l$. We have $p_0 = 1/6, p_1 = 3/6=1/2$ and $p_2 = 2/6 = 1/3$. This means we have the following entropy: \n",
    "$$\n",
    "H(left leaf) = -\\left(\\frac{1}{6} log_2\\left(\\frac{1}{6}\\right) + \\frac{3}{6} log_2\\left(\\frac{3}{6}\\right) + \\frac{2}{6} log_2\\left(\\frac{2}{6}\\right)\\right) = 1.459\n",
    "$$\n",
    "\n",
    "#### Right leaf entropy\n",
    "The remaining data is:\n",
    "$$ X = \\begin{bmatrix}\n",
    "  33. \\\\\n",
    "  20. \\\\\n",
    "  27. \\\\\n",
    "  \\end{bmatrix}\n",
    ", \\quad\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  0 \\\\ \n",
    "\\end{bmatrix} $$\n",
    "And the entropy here is a bit easier to calculate. We have $p_0 = 1$ which means: \n",
    "$$\n",
    "H(right leaf) = -\\left(1 log_2\\left(1\\right) \\right) = 0\n",
    "$$\n",
    "\n",
    "#### Total tree entropy\n",
    "We calculate the total entropy by adding the 2 previous entropies multiplied by their weight, which is how many of the data points fell their way. So this is $p_l = 6/9$ and $p_r=3/9$. So we have: \n",
    "$$\n",
    "H(T) = \\frac{6}{9} \\cdot 1.459 + \\frac{3}{9} \\cdot 0 = 0.972\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4591479170272446, -0.0, 0.972765278018163)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy(probs):\n",
    "    \"\"\"Calculate the entropy given a list of probabilities.\"\"\"\n",
    "    return -sum(p * np.log2(p) for p in probs if p > 0)\n",
    "\n",
    "#Data from the problem\n",
    "X = np.array([9, 33, 20, 27, 3, 6, 18, 14, 16])\n",
    "Y = np.array([0, 0, 0, 0, 1, 1, 1, 2, 2])\n",
    "\n",
    "#Split the data at X < 19\n",
    "left_leaf_indices = X < 19\n",
    "right_leaf_indices = X >= 19\n",
    "\n",
    "#Left Leaf Labels\n",
    "left_labels = Y[left_leaf_indices]\n",
    "#Right Leaf Labels\n",
    "right_labels = Y[right_leaf_indices]\n",
    "\n",
    "#Count the number of occurrences of each class in the left leaf\n",
    "left_counts = np.bincount(left_labels, minlength=3)\n",
    "left_probs = left_counts / len(left_labels)\n",
    "H_left = entropy(left_probs)\n",
    "\n",
    "#Count the number of occurrences of each class in the right leaf\n",
    "right_counts = np.bincount(right_labels, minlength=3)\n",
    "right_probs = right_counts / len(right_labels)\n",
    "H_right = entropy(right_probs)\n",
    "\n",
    "#Calculate the overall entropy of the tree\n",
    "n_left = len(left_labels)\n",
    "n_right = len(right_labels)\n",
    "n_total = len(Y)\n",
    "\n",
    "H_tree = (n_left / n_total) * H_left + (n_right / n_total) * H_right\n",
    "\n",
    "H_left, H_right, H_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Implementing Regression Stumps\n",
    "In this exercise your task is to implement Regression Trees that consist of one internal node (the root) and two leafs.\n",
    "Such trees are known as Regression Stumps.\n",
    "For the loss/cost function we consider least squares loss $(h(x) - y)^2$ \n",
    "\n",
    "This means that the learning algorithm has to find the best possible feature to split the training data using a single feature value pair in regards to Least Squares loss.\n",
    "\n",
    "We have decided for you to present a Regression Stump by \n",
    "- idx: the data/feature vector index to consider in the root node (the one question asked)\n",
    "- val: the value to compare to for data feature idx in the root node\n",
    "- left: the value to return for a data point if it ends up in left leaf (x[idx] < val) (only question type we consider in a node)\n",
    "- right: the value to return for a data point if it ends up in the right leaf (x[idx] >= val)\n",
    "\n",
    "The approach we follow is as follows. Assume the input data has n data points each a vector of $d$ real numbers.\n",
    "\n",
    "**Basic Algorithm:**\n",
    "\n",
    "For each data feature f:\n",
    "-   Compute for all possible values $v$ for feature $f$ in the training data, the least squares cost of the stump achieved by using feature f and value $v$ in the root using the optimal value in the two leafs. This gives a list of of costs, one for each split ($f, v: \\textrm{cost}$). \n",
    "- Pick the split $f, v$ with minimal cost and create the corresponding tree by setting idx, val, left, right\n",
    " \n",
    "Your task is to give a full implementation of this algorithm and specify the running time.\n",
    "\n",
    "**hint:** It is fine to implement a simple version for finding the best split that takes $O(d n^2)$ time.\n",
    "\n",
    "See **regression_stumps.py** for starter code.\n",
    "\n",
    "**You need to complete the RegressionStump class by completing the following methods**\n",
    "- implement predict \n",
    "- implement score\n",
    "- implement fit\n",
    "\n",
    "We advice to implement in the order specified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. BONUS exercise if time: Data that is not numbers\n",
    "### Question 1: Spam Filters\n",
    "You are given the task to design a spam filter and you will be using **Linear Classification** and the perceptron algorithm (since that, and decision trees, is all we know yet).\n",
    "\n",
    "The input data consists of a list of (email, spam/not spam label), and each email is represented by a variable length text string. \n",
    "Can you train a spam filter using this data using the perceptron algorithm and if so how? What issues do you see and do you have any ideas how they could be adressed? \n",
    "\n",
    "\n",
    "### Question 2: Categorical Features\n",
    "You are solving a problem with machine learning and have decided to use linear classification (Perceptron). \n",
    "One of the data features is categorical and has four unordered values: Apple, Banana, Grape, Mango. \n",
    "\n",
    "How could you use that feature in a linear classification setup? (The data should be a matrix of size $n \\times d$ of real numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7 solution \n",
    "#### Question 1\n",
    "\n",
    "\n",
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. BONUS exercise if time: Classification Stumps in O(n d lg n ) time\n",
    "In this exercise your job is to describe an algorithm that given a data set of labelled data (two classses only), constructs the binary classification tree (one internal node and two leafs) that minimize the 0-1 Loss over the training data. Such small classification trees are called classification stumps.\n",
    "\n",
    "i.e. given data \n",
    "$$ D = \\{(x_i, y_i) \\mid 1\\leq i \\leq n, y_i \\in \\{0, 1\\}, x_i \\in \\mathbb{R}^d\\}$$\n",
    "construct the binary classification tree $T$ that minimize \n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n 1_{T(x_i) \\neq y_i}\n",
    "$$\n",
    "Your algorithm must only use $O(n d \\log n)$ time. \n",
    "\n",
    "The root node considers only questions like $f_i < 42$ and this may be represented by the feature's index i and the value to compare with (42 here).\n",
    "\n",
    "\n",
    "**Hint: Consider each feature in turn and sort the data for that feature and permute the labels $y$ with the same ordering and compute the score for each relevant split in $O(n \\lg n)$ time**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
