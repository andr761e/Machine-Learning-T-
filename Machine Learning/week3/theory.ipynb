{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Exercises week 3\n",
    "**Like last week, it is very imporant that you try to solve every exercise. \n",
    "If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n",
    "\n",
    "\n",
    "The TA's will be very happy to answer questions during the TA session or on the board.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feasibility of Learning\n",
    "\n",
    "\n",
    "## Exercise 1.12 from [LFD]:\n",
    "A friend comes to you with a learning problem. She says the target function is <i>completely</i> unknown, but she has 4000 data points. She is willing to pay you to solve her problem and produce for her a $g$ which approximates $f$. What is the best that you can promise her among the following:\n",
    "\n",
    "<b>a</b> After learning you will provide her with a $g$ that you will guarantee approximates $f$ well out of sample.\n",
    "\n",
    "<b>b</b> After learning you will provide her with a $g$, and with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "<b>c</b> One of two things will happen.\n",
    "<b>(i)</b> You will produce a hypothesis $g$;\n",
    "<b>(ii)</b> You will declare that you failed.\n",
    "If you do return a hypothesis $g$, then with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 answer\n",
    "I think the best I can promise is (c).\n",
    "\n",
    "-The unknown target $f$ can be very complex and maybe we can't learn it at all.\n",
    "\n",
    "-If we can learn and produce a hypothesis $g$, since there are many data points (4000), the probability that $g$ matches $f$ is high according to Hoeffding inequality, and the error on $g$ might be small since we have a large data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. In-Sample and Out-of-Sample Error\n",
    "\n",
    "Consider an input domain $X=\\{x_1,x_2,x_3,x_4\\}$. Let the unknown target function $f$ assign labels $1$ to all $4$ elements in $X$.\n",
    "\n",
    "Consider a hypothesis $h$ such that $h(x_1)=h(x_2)=h(x_3)=1$ and $h(x_4)=-1$.\n",
    "\n",
    "<b>Question 1</b>: What is the worst possible $E_{in}(h)$ over any data set $S$ of $5$ samples of the form $(x,f(x))$ where $x \\in X$?\n",
    "\n",
    "<b>Question 2</b>: What is the best possible $E_{in}(h)$ over any data set $S$ of $5$ samples of the form $(x,f(x))$ where $x \\in X$?\n",
    "\n",
    "<b>Question 3</b>: Consider an unknown distribution $D$, such that $D(x_1)=D(x_2)=D(x_3)=1/6$ and $D(x_4)=1/2$ (here $D(x_i)$ denotes the probability of seeing the sample $x_i$). What is $E_{out}(h)$ under this distribution? What is the worst possible $E_{in}(h)$ we could get if we sample $5$ elements from $D$? What is the best $E_{in}(h)$?\n",
    "\n",
    "<b>Question 4</b>: Give an example of a distribution $D$ over $X$ such that $E_{out}(h)=1$.\n",
    "\n",
    "<b>Question 5</b>: Give an example of a distribution $D$ over $X$ such that $E_{out}(h)=0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 answer\n",
    "### Question 1\n",
    "The worst scenario for $E_{in}(h)$ occurs when the hypothesis disagress with the target function $f$. We know that $h(x_4)=-1$ so if we had a set only containing $x_4$ 5 times and $f(x_4)=1$ then it would be completely wrong and we would have in-sample error $E_{in}(h) = \\frac{5}{5}=1$ which is the worst possible. \n",
    "\n",
    "### Question 2\n",
    "The hypothesis agrees with the traget function for $x_1, x_2$ and $x_3$ so if we have a sample without $x_4$ we get $E_{in}(h) = \\frac{0}{5}=0$ which is the best possible. \n",
    "\n",
    "### Question 3\n",
    "We have probability 0.5 of getting one of the first 3 samples that are matching and probability 0.5 of getting $x_4$ which does not match. So with this distribution we expect: \n",
    "$$\n",
    "E_{out}(h) = \\frac{1}{2}1 + \\frac{1}{2}0 = 0.5\n",
    "$$\n",
    "The worst possible scenario is sampling all $x_4$. This would give us $E_{in}=1$ and would happen with probability $(0.5)^5 = 1/32$.\n",
    "\n",
    "And vice versa, the best is to sample anything but $x_4$. This would give us $E_{in}=0$ and would happen with probability $(0.5)^5 = 1/32$.\n",
    "\n",
    "### Question 4\n",
    "This can only happen if $D(x_4) = 1$ as we would need to only sample $x_4$ every time. \n",
    "\n",
    "### Question 5\n",
    "And this is the opposite scenario where we would need to have $D(x_4) = 0$. The distribution of the remaining elements does not matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generalization\n",
    "\n",
    "\n",
    "## Questions:\n",
    "The Hoeffding bound gives us the following guarantee:\n",
    "$$\n",
    "\\Pr[|E_{in}-E_{out}| > \\varepsilon] \\leq 2Me^{-2\\varepsilon^2 n},\n",
    "$$\n",
    "where the probability is over the random choice of the sample.\n",
    "\n",
    "<b>Question 1: </b> \n",
    "Does the Hoeffding bound give any meaningful bounds on $E_{in}$ and $E_{out}$ for the perceptron learning model?  \n",
    "\n",
    "<b>Question 2: </b> \n",
    "Assume you use a hypothesis set $H$ with $|H|=M=10^6$. How many samples $n$ do you need to guarantee that $|E_{in} - E_{out}| < 0.1$ with probability at least $0.95$? \n",
    "\n",
    "<b>Question 3: </b> \n",
    "Assume you use a hypothesis set $H$ with $|H|=M=10^6$ and you have $n=10^4$ samples. How small can you guarantee that $|E_{in} - E_{out}|$ is with probability at least $0.95$? \n",
    "\n",
    "<b>Question 4: </b> \n",
    "Assume you have $n=10^4$ samples and would like to guarantee that $|E_{in} - E_{out}|<0.2$ with probability at least $0.95$. How large a hypothesis set can you use?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 answer\n",
    "We have that $|H| = M$ is the size of the hypothesis set. $n$ is the number of samples and $\\epsilon$ is the tolerance for the difference in $E_{in}$ and $E_{out}$. \n",
    "\n",
    "### Question 1\n",
    "Perceptron er lavet ud fra vektorer (linjer) i fx $\\mathbb{R}^2$ og derved ville man få $M = \\infty$ i uligheden, hvilket ikke giver mening. \n",
    "\n",
    "### Question 2\n",
    "We want \n",
    "$$\n",
    "\\Pr[|E_{in}-E_{out}| > \\varepsilon] \\leq 0.05\n",
    "$$\n",
    "and we have $M = 10^6$ and $\\epsilon=0.1$ and we want to be 95\\% sure. We therefore solve: \n",
    "$$\n",
    "2 \\cdot 10^6 \\cdot e^{-2 \\cdot 0.1^2 n} \\leq 0.05 \\Rightarrow n \\geq 875.2195006 \n",
    "$$\n",
    "We round up to 876 because samples are integers. \n",
    "\n",
    "### Question 3\n",
    "We now have $M=10^6$, $n=10^4$ and we want to be 95\\% sure. We have: \n",
    "$$\n",
    "2 \\cdot 10^6 \\cdot e^{-2 \\cdot \\varepsilon^2 \\cdot 10^4} \\leq 0.05 \\Rightarrow \\varepsilon \\geq 0.02958410892\n",
    "$$\n",
    "\n",
    "### Question 4\n",
    "We now have $n=10^4$, $\\varepsilon=0.2$ and we want to be 95\\% sure. We have: \n",
    "$$\n",
    "2 \\cdot M \\cdot e^{-2 \\cdot 0.2^2 \\cdot 10^4} \\leq 0.05 \\Rightarrow M \\leq 6.815936431*10^{345}\n",
    "$$\n",
    "Which is an absurd number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Noisy Targets\n",
    "In the lectures, we extended the supervised learning setup to a case where the labels are noisy. Formally, for any feature vector $x$, there is a distribution $P(y \\mid x)$ on the label $y$.\n",
    "\n",
    "Assume the data distribution $D$ gives a uniform random feature vector among a fixed set of three vectors $x_1,x_2$ and $x_3$.\n",
    "\n",
    "Assume that $Pr(y=1 \\mid x_1) = 2/3$ (and thus $Pr(y=-1 \\mid x_1) = 1/3$). Also, assume $Pr(y=1 \\mid x_2) = 1/2$ and $Pr(y=1 \\mid x_3) = 1/4$.\n",
    "\n",
    "<b>Question</b>: What is the best possible out-of-sample error that any hypothesis $h : \\{x_1,x_2,x_3\\} \\to \\{-1,1\\}$ can achieve? And what are the predictions made by that hypothesis $h$?\n",
    "\n",
    "<b>Question (*A bit hard, so maybe skip)</b>: Does it help to allow a <i>randomized</i> hypothesis? A randomized hypothesis $h$ is one such that on any given $x$, we randomly output $1$ with probability $p_{h,x}$ and $-1$ with probability $1-p_{h,x}$. Here the probability $p_{h,x}$ thus depends on both $h$ and $x$. \n",
    "\n",
    "Hint: Try to look at an example where $Pr(y=1 \\mid x)=p$ for a $p \\geq 1/2$ and assume a randomized hypothesis $h$ predicts $1$ with probability $p_{h,x}$. Then determine the best choice of $p_{h,x}$ to minimize the chance of mispredicting the label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 answer\n",
    "\n",
    "### Question \n",
    "We want to form our hypothesis such that we predict them on whats most likely so we have $h(x_1) = 1$, $h(x_2) = 1$ (but could also be -1 because they are equally likely) and $h(x_3)=-1$.\n",
    "\n",
    "We then calculate the out-of-sample error knowing their probabilities of being mis matched and assuming that they are evenly distributed: \n",
    "$$\n",
    "E_{out}(h) = \\frac{1}{3} \\left(\\frac{1}{3} +\\frac{1}{2} +\\frac{1}{4}  \\right) = \\frac{13}{36} = 0.361\n",
    "$$\n",
    "\n",
    "### Hard Question\n",
    "Chancen for at $x_1$ er 1 er 2/3 og chancen for at det er -1 er 1/3. Chancen for at hypotesen er 1 er $p_{x_1,h}$ og chancen for at den er -1 er $1-p_{x_1,h}$. Så den samlede chance for at den gætter rigtigt er $\\frac{2}{3}p_{x_1,h} + (1-p_{x_1,h})\\frac{1}{3} = p_{x_1,h}\\frac{1}{3} + \\frac{1}{3}$. Vi gør dette for alle og får $\\frac{1}{2}p_{x_2,h} + (1-p_{x_2,h})\\frac{1}{2} = \\frac{1}{2}$ og $\\frac{1}{4}p_{x_2,h} + (1-p_{x_3,h})\\frac{3}{4} = -p_{x_3,h} \\frac{1}{2} + \\frac{3}{4}$. Vi gør ligesom i det nemme spørgsmål og tager den inverse sandsynlighed som er chancen for mis match, ligger dem sammen og ganger med 1/3 fordi de er evenly distributed: \n",
    "$$\n",
    "E_{out}(h) = \\frac{1}{3} \\left( (1-p_{x_1,h}\\frac{1}{3} + \\frac{1}{3}) + \\frac{1}{2} + (1-p_{x_3,h}\\frac{1}{3} + \\frac{1}{3})  \\right) = \\frac{17}{36} - \\frac{p_{x_1,h}}{9} + \\frac{p_{x_3,h}}{6}\n",
    "$$\n",
    "Vi minimerer dette ved at sætte $p_{x_1,h} = 1$ og $p_{x_3,h} = 0$. Vi får: \n",
    "$$\n",
    "E_{out}(h) = \\frac{17}{36} - \\frac{1}{9} + \\frac{0}{6} = \\frac{13}{36}\n",
    "$$\n",
    "Hvilket er det samme som før. Så vi kan ikke få et bedre resultat på denne måde. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Regression and the missing inverse\n",
    "In linear regression, given data matrix $X$ and labels vector $y$ the optimal weight vector $w$ (minimizing $\\|Xw-y\\|_2^2$), is found simply by computing the matrix product\n",
    "$$\n",
    "(X^\\intercal X)^{-1}X^\\intercal y\n",
    "$$\n",
    "That only makes sense if $(X^\\intercal X)$ is in fact invertible.\n",
    "\n",
    "In the lectures I suggested that if indeed $(X^\\intercal X)$ is not invertible then we should remove linear dependent columns from $X$.\n",
    "In this exercise you must argue that this is a good idea.\n",
    "\n",
    "To do this, you must prove/argue the two following things\n",
    "* If $(X^\\intercal X)$ is not invertible then $X$ contains linearly dependent columns\n",
    "* Removing linear dependent columns from X does not change the cost of an optimal solution $w$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HINT 1: you can use a well known linear algebra fact that rank(X) = rank($X^\\intercal X$) = rank($X X^\\intercal$)\n",
    "\n",
    "HINT 2: $Xw$ is in the column space of $X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 answer\n",
    "\n",
    "### Part 1: If $ (X^\\top X) $ is not invertible, then $ X $ contains linearly dependent columns\n",
    "\n",
    "- The rank of $ X^\\top X $ is equal to the rank of $ X $, i.e., $ \\text{rank}(X^\\top X) = \\text{rank}(X) $.\n",
    "- If $ X $ has linearly dependent columns, then $ \\text{rank}(X) < \\text{num\\_columns}(X) $, meaning $ X $ is not full rank.\n",
    "- Since $ X^\\top X $ shares the rank with $ X $, $ X^\\top X $ also has reduced rank and is thus **singular** (i.e., not invertible).\n",
    "\n",
    "### Part 2: Removing linearly dependent columns from $ X $ does not change the cost of the optimal solution $ w $\n",
    "\n",
    "- The vector $ Xw $ lies in the **column space** of $ X $.\n",
    "- Linearly dependent columns do not add any new information to the column space, so removing them doesn’t change the span of the space.\n",
    "- The cost function $ \\|Xw - y\\|^2 $ is minimized over this column space, and removing dependent columns doesn’t affect the column space, so it does not change the optimal solution.\n",
    "  \n",
    "### Conclusion\n",
    "\n",
    "- If $ (X^\\top X) $ is not invertible, $ X $ has linearly dependent columns.\n",
    "- Removing these dependent columns simplifies the matrix without affecting the column space or the optimal solution for $ w $, and thus the cost remains unchanged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Matrix of Derivatives\n",
    "\n",
    "In Linear Regression we define the in sample error as (ignoring the normalizing factor 1/n)\n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 $$ \n",
    "\n",
    "Let $X$ be the the data matrix of shape $n \\times d$ with data point $x_i$ as the $i$'th row. Let $y$ be the label vector of shape $n \\times 1$ with label $y_i$ as the $i$'th entry. Let $w$ be the weight vector of shape $d \\times 1$.  \n",
    "\n",
    "$$X=\\begin{pmatrix}\n",
    "- & x_1^T & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & x_n^T & - \\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times d}\\quad\\quad\\quad\n",
    "y=\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n \\times 1}$$\n",
    "\n",
    "The in-sample error rate $E_{in}$ is then equal to \n",
    "\n",
    "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 = \\|Xw-y\\|^2 = (Xw-y)^\\intercal (Xw-y)$$\n",
    "\n",
    "\n",
    "In the lecture we proved that for Linear Regression the optimal weight vector $w_\\textrm{lin}$ for minimizing $E_\\textrm{in}$ was $w_\\textrm{lin}=(X^\\intercal X)^{-1} X^\\intercal y$. \n",
    "\n",
    "To do this we used facts about the derivatives. \n",
    "* If we have a function $f(z): \\mathbb{R}^{a} \\rightarrow \\mathbb{R}^b$ such that $f(z) = [f_1(z),\\dots, f_b(z)]$ then the matrix of derivatives $\\frac{\\partial f}{\\partial z}$ is of size $b\\times a$ where\n",
    "$$ \\left[\\frac{\\partial f}{\\partial z} \\right]_{i,j} = \\frac{\\partial f_i}{\\partial z_j} $$\n",
    "\n",
    "For example: Let $f(x): R^2 \\rightarrow R^3$ be the function $f([x_1, x_2]) = [x_1, x_2, x_1 \\cdot x_2]$ then \n",
    "the matrix of derivatives has shape $3 \\times 2$ and looks like $ \\frac{\\partial f}{\\partial x} =\n",
    " \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1  \\\\\n",
    "  x_2  & x_1 \\\\\n",
    " \\end{bmatrix} $\n",
    "\n",
    "In our proof we used the following identities about the matrix of derivatives\n",
    "\n",
    "* $f: R^d \\rightarrow R^n, f(z) = Xz-y$, the matrix of derivatives $\\frac{\\partial f}{\\partial z}$ is $X$\n",
    "* $g: R^d \\rightarrow R, g(z) = z^\\intercal z$, the matrix of derivatives $\\frac{\\partial g}{\\partial z}$ is $2z^\\intercal$\n",
    "\n",
    "\n",
    "## Your job is to prove the two identities. \n",
    "\n",
    "* Let $f(z) = Xz - y$, Where $X$ is a $n \\times d$ matrix, $y$ is a $n\\times 1$ vector and $z$ is a $d \\times 1$ vector (function from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^n$). \n",
    "Show that the matrix of derivatives of $f$ is X. \n",
    "\n",
    "Hint: You can think of $f = [f_1,\\dots,f_n]$ as n output functions where $f_i(z) = x_{i}^\\intercal z - y_i$ and $x_i$ is the i'th row of $X$ (as a column vector) and $y_i$ is the i'th entry in vector $y$. Start with $\\frac{\\partial f_1}{\\partial z_1}$ to see if a pattern emerges\n",
    "* Let $g(z) = z^\\intercal z$ where z is a vector (the squared norm of $z$). Show that the matrix of derivatives is $2z^\\intercal$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Implementing Linear Regression\n",
    "\n",
    "In this exercise your task is to implement Linear Regression.\n",
    "\n",
    "See **linear_regression.py** for starter code.\n",
    "\n",
    "**You need to complete the LinearRegressor class by completing the following methods**\n",
    "- implement hardcode_bias \n",
    "- implement predict \n",
    "- implement score\n",
    "- implement fit"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
